use anyhow::{Context, Result};
use fastembed::{EmbeddingModel, TextEmbedding, InitOptions};
use log::{info, debug};

#[derive(serde::Serialize)]
struct OpenAIEmbeddingRequest {
    input: Vec<String>,
    model: String,
}

#[derive(serde::Deserialize)]
struct OpenAIEmbeddingData {
    embedding: Vec<f64>,
}

#[derive(serde::Deserialize)]
struct OpenAIEmbeddingResponse {
    data: Vec<OpenAIEmbeddingData>,
}

pub enum EmbeddingProvider {
    Local(TextEmbedding),
    Remote {
        client: reqwest::Client,
        api_key: String,
        model: String,
        endpoint: String,
    },
}

/// Embedding manager supporting both local FastEmbed and remote API models
pub struct EmbeddingManager {
    provider: EmbeddingProvider,
    model_name: String,
}

impl EmbeddingManager {
    pub async fn new(model_name: &str) -> Result<Self> {
        info!("Initializing FastEmbed model: {}", model_name);
        
        // Map model names to FastEmbed EmbeddingModel variants
        let embedding_model = match model_name {
            "BAAI/bge-small-en-v1.5" => EmbeddingModel::BGESmallENV15,
            "BAAI/bge-base-en-v1.5" => EmbeddingModel::BGEBaseENV15,
            "BAAI/bge-large-en-v1.5" => EmbeddingModel::BGELargeENV15,
            "sentence-transformers/all-MiniLM-L6-v2" => EmbeddingModel::AllMiniLML6V2,
            "sentence-transformers/all-MiniLM-L12-v2" => EmbeddingModel::AllMiniLML12V2,
            "intfloat/multilingual-e5-large" => EmbeddingModel::MultilingualE5Large,
            "intfloat/e5-large-v2" => EmbeddingModel::BGELargeENV15, // Fallback to similar model
            _ => {
                info!("Model '{}' not directly supported, using BGE-small-en-v1.5 as fallback", model_name);
                EmbeddingModel::BGESmallENV15
            }
        };
        
        info!("Loading model with download progress and Metal acceleration...");
        
        // Configure ONNX Runtime to use Metal/CoreML when available on macOS
        #[cfg(target_os = "macos")]
        {
            // Set execution providers in order of preference
            std::env::set_var("ORT_EXECUTION_PROVIDERS", "CoreMLExecutionProvider,CPUExecutionProvider");
            
            // Additional CoreML optimizations
            std::env::set_var("ORT_COREML_FLAGS", "COREML_FLAG_USE_CPU_AND_GPU");
            
            info!("üöÄ Configured ONNX Runtime for Apple Silicon:");
            info!("   ‚Ä¢ CoreML (Metal GPU) acceleration enabled");
            info!("   ‚Ä¢ CPU fallback available");
            info!("   ‚Ä¢ Using both CPU and GPU for CoreML");
        }
        
        #[cfg(not(target_os = "macos"))]
        {
            info!("‚ÑπÔ∏è  Non-macOS platform detected, using default CPU execution");
        }
        
        let model = TextEmbedding::try_new(
            InitOptions::new(embedding_model).with_show_download_progress(true)
        ).context("Failed to initialize FastEmbed model")?;
        
        info!("FastEmbed model loaded successfully: {}", model_name);
        
        Ok(Self {
            provider: EmbeddingProvider::Local(model),
            model_name: model_name.to_string(),
        })
    }
    
    pub async fn new_remote(api_key: String, model_name: &str, endpoint: Option<String>) -> Result<Self> {
        let endpoint = endpoint.unwrap_or_else(|| "https://api.openai.com/v1/embeddings".to_string());
        info!("Initializing remote embedding model: {} at {}", model_name, endpoint);
        
        let client = reqwest::Client::new();
        
        Ok(Self {
            provider: EmbeddingProvider::Remote {
                client,
                api_key,
                model: model_name.to_string(),
                endpoint,
            },
            model_name: model_name.to_string(),
        })
    }
    
    pub async fn generate_embedding(&mut self, text: &str) -> Result<Vec<f64>> {
        if text.trim().is_empty() {
            anyhow::bail!("Cannot generate embedding for empty text");
        }
        
        debug!("Generating embedding for text of length: {}", text.len());
        
        let embedding = match &mut self.provider {
            EmbeddingProvider::Local(model) => {
                let embeddings = model
                    .embed(vec![text], None)
                    .context("Failed to generate embedding with FastEmbed")?;
                
                if embeddings.is_empty() {
                    anyhow::bail!("No embeddings generated by FastEmbed");
                }
                
                // Convert Vec<f32> to Vec<f64> for database storage
                embeddings[0].iter().map(|&x| x as f64).collect()
            },
            EmbeddingProvider::Remote { client, api_key, model, endpoint } => {
                let request = OpenAIEmbeddingRequest {
                    input: vec![text.to_string()],
                    model: model.clone(),
                };
                
                let response = client
                    .post(endpoint.as_str())
                    .header("Authorization", format!("Bearer {}", api_key))
                    .header("Content-Type", "application/json")
                    .json(&request)
                    .send()
                    .await
                    .context("Failed to send request to remote embedding API")?;
                
                if !response.status().is_success() {
                    let error_text = response.text().await.unwrap_or_default();
                    anyhow::bail!("Remote embedding API error: {}", error_text);
                }
                
                let openai_response: OpenAIEmbeddingResponse = response
                    .json()
                    .await
                    .context("Failed to parse remote embedding API response")?;
                
                if openai_response.data.is_empty() {
                    anyhow::bail!("No embeddings returned by remote API");
                }
                
                openai_response.data[0].embedding.clone()
            }
        };
        
        debug!("Generated embedding with dimension: {}", embedding.len());
        Ok(embedding)
    }

    /// Generate embeddings for multiple texts in a single batch for improved performance
    pub async fn generate_embeddings_batch(&mut self, texts: &[String]) -> Result<Vec<Vec<f64>>> {
        if texts.is_empty() {
            return Ok(Vec::new());
        }
        
        // Filter out empty texts and track their original positions
        let mut valid_texts = Vec::new();
        let mut valid_indices = Vec::new();
        
        for (i, text) in texts.iter().enumerate() {
            if !text.trim().is_empty() {
                valid_texts.push(text.as_str());
                valid_indices.push(i);
            }
        }
        
        let valid_count = valid_texts.len();
        
        if valid_count == 0 {
            anyhow::bail!("Cannot generate embeddings for empty texts");
        }
        
        debug!("Generating embeddings for batch of {} texts", valid_count);
        
        let embeddings = match &mut self.provider {
            EmbeddingProvider::Local(model) => {
                let embeddings = model
                    .embed(valid_texts, None)
                    .context("Failed to generate batch embeddings with FastEmbed")?;
                
                if embeddings.len() != valid_count {
                    anyhow::bail!(
                        "Embedding count mismatch: expected {}, got {}", 
                        valid_count, 
                        embeddings.len()
                    );
                }
                
                // Convert from Vec<f32> to Vec<f64>
                embeddings.into_iter()
                    .map(|embedding| embedding.iter().map(|&x| x as f64).collect())
                    .collect::<Vec<Vec<f64>>>()
            },
            EmbeddingProvider::Remote { client, api_key, model, endpoint } => {
                let input_texts: Vec<String> = valid_texts.iter().map(|s| s.to_string()).collect();
                
                let request = OpenAIEmbeddingRequest {
                    input: input_texts,
                    model: model.clone(),
                };
                
                let response = client
                    .post(endpoint.as_str())
                    .header("Authorization", format!("Bearer {}", api_key))
                    .header("Content-Type", "application/json")
                    .json(&request)
                    .send()
                    .await
                    .context("Failed to send batch request to remote embedding API")?;
                
                if !response.status().is_success() {
                    let error_text = response.text().await.unwrap_or_default();
                    anyhow::bail!("Remote embedding API error: {}", error_text);
                }
                
                let openai_response: OpenAIEmbeddingResponse = response
                    .json()
                    .await
                    .context("Failed to parse remote embedding API response")?;
                
                if openai_response.data.len() != valid_count {
                    anyhow::bail!(
                        "Embedding count mismatch: expected {}, got {}", 
                        valid_count, 
                        openai_response.data.len()
                    );
                }
                
                openai_response.data.into_iter()
                    .map(|data| data.embedding)
                    .collect()
            }
        };
        
        // Map embeddings back to their original positions
        let mut result = vec![Vec::new(); texts.len()];
        for (valid_idx, &embedding_idx) in valid_indices.iter().enumerate() {
            result[embedding_idx] = embeddings[valid_idx].clone();
        }
        
        debug!("Generated {} embeddings with dimension: {}", 
               embeddings.len(), 
               embeddings.first().map(|e| e.len()).unwrap_or(0));
        
        Ok(result)
    }
    
    pub fn model_name(&self) -> &str {
        &self.model_name
    }
    
    pub fn embedding_dimension(&self) -> usize {
        // Get dimension from the actual model
        // This is a rough estimate based on common model dimensions
        match self.model_name.as_str() {
            "BAAI/bge-small-en-v1.5" => 384,
            "BAAI/bge-base-en-v1.5" => 768,
            "BAAI/bge-large-en-v1.5" => 1024,
            "sentence-transformers/all-MiniLM-L6-v2" => 384,
            "sentence-transformers/all-MiniLM-L12-v2" => 384,
            "intfloat/multilingual-e5-large" => 1024,
            _ => 384, // Default fallback
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_embedding_generation() {
        // Note: These tests require model downloads, so they may be slow on first run
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let text = "This is a test sentence for embedding generation.";
        
        let embedding = manager.generate_embedding(text).await.unwrap();
        
        assert!(!embedding.is_empty());
        assert_eq!(embedding.len(), manager.embedding_dimension());
        
        // Check that embedding values are reasonable (not all zeros)
        assert!(embedding.iter().any(|&x| x != 0.0));
        
        // FastEmbed embeddings are typically normalized
        let magnitude: f64 = embedding.iter().map(|x| x * x).sum::<f64>().sqrt();
        assert!(magnitude > 0.5); // Should be reasonably normalized
    }
    
    #[tokio::test]
    async fn test_consistent_embeddings() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let text = "This is a test sentence.";
        
        let embedding1 = manager.generate_embedding(text).await.unwrap();
        let embedding2 = manager.generate_embedding(text).await.unwrap();
        
        // FastEmbed should produce consistent embeddings for the same input
        assert_eq!(embedding1, embedding2);
    }
    
    #[tokio::test]
    async fn test_different_texts_different_embeddings() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        
        let embedding1 = manager.generate_embedding("First text").await.unwrap();
        let embedding2 = manager.generate_embedding("Second text").await.unwrap();
        
        // Different texts should produce different embeddings
        assert_ne!(embedding1, embedding2);
    }
    
    #[tokio::test]
    async fn test_empty_text_error() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let result = manager.generate_embedding("").await;
        
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_batch_embedding_generation() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let texts = vec![
            "First test sentence.".to_string(),
            "Second test sentence.".to_string(),
            "Third test sentence.".to_string(),
        ];
        
        let embeddings = manager.generate_embeddings_batch(&texts).await.unwrap();
        
        assert_eq!(embeddings.len(), texts.len());
        
        // Check that each embedding has the right dimension
        for embedding in &embeddings {
            assert!(!embedding.is_empty());
            assert_eq!(embedding.len(), manager.embedding_dimension());
        }
        
        // Check that different texts produce different embeddings
        assert_ne!(embeddings[0], embeddings[1]);
        assert_ne!(embeddings[1], embeddings[2]);
    }

    #[tokio::test]
    async fn test_batch_with_empty_texts() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let texts = vec![
            "Valid text".to_string(),
            "".to_string(),  // Empty text
            "Another valid text".to_string(),
        ];
        
        let embeddings = manager.generate_embeddings_batch(&texts).await.unwrap();
        
        assert_eq!(embeddings.len(), texts.len());
        assert!(!embeddings[0].is_empty());  // First should have embedding
        assert!(embeddings[1].is_empty());   // Second should be empty (was empty text)
        assert!(!embeddings[2].is_empty());  // Third should have embedding
    }
}
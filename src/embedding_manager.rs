use anyhow::{Context, Result};
use fastembed::{EmbeddingModel, TextEmbedding, InitOptions};
use log::{info, debug, warn};

/// FastEmbed-based embedding manager for generating real embeddings
pub struct EmbeddingManager {
    model: TextEmbedding,
    model_name: String,
}

impl EmbeddingManager {
    pub async fn new(model_name: &str) -> Result<Self> {
        info!("Initializing FastEmbed model: {}", model_name);
        
        // Map model names to FastEmbed EmbeddingModel variants
        let embedding_model = match model_name {
            "BAAI/bge-small-en-v1.5" => EmbeddingModel::BGESmallENV15,
            "BAAI/bge-base-en-v1.5" => EmbeddingModel::BGEBaseENV15,
            "BAAI/bge-large-en-v1.5" => EmbeddingModel::BGELargeENV15,
            "sentence-transformers/all-MiniLM-L6-v2" => EmbeddingModel::AllMiniLML6V2,
            "sentence-transformers/all-MiniLM-L12-v2" => EmbeddingModel::AllMiniLML12V2,
            "intfloat/multilingual-e5-large" => EmbeddingModel::MultilingualE5Large,
            "intfloat/e5-large-v2" => EmbeddingModel::BGELargeENV15, // Fallback to similar model
            _ => {
                info!("Model '{}' not directly supported, using BGE-small-en-v1.5 as fallback", model_name);
                EmbeddingModel::BGESmallENV15
            }
        };
        
        info!("Loading model with download progress and Metal acceleration...");
        
        // Configure ONNX Runtime to use Metal/CoreML when available on macOS
        #[cfg(target_os = "macos")]
        {
            // Set execution providers in order of preference
            std::env::set_var("ORT_EXECUTION_PROVIDERS", "CoreMLExecutionProvider,CPUExecutionProvider");
            
            // Additional CoreML optimizations
            std::env::set_var("ORT_COREML_FLAGS", "COREML_FLAG_USE_CPU_AND_GPU");
            
            info!("🚀 Configured ONNX Runtime for Apple Silicon:");
            info!("   • CoreML (Metal GPU) acceleration enabled");
            info!("   • CPU fallback available");
            info!("   • Using both CPU and GPU for CoreML");
        }
        
        #[cfg(not(target_os = "macos"))]
        {
            info!("ℹ️  Non-macOS platform detected, using default CPU execution");
        }
        
        let model = TextEmbedding::try_new(
            InitOptions::new(embedding_model).with_show_download_progress(true)
        ).context("Failed to initialize FastEmbed model")?;
        
        info!("FastEmbed model loaded successfully: {}", model_name);
        
        Ok(Self {
            model,
            model_name: model_name.to_string(),
        })
    }
    
    pub async fn generate_embedding(&mut self, text: &str) -> Result<Vec<f64>> {
        if text.trim().is_empty() {
            anyhow::bail!("Cannot generate embedding for empty text");
        }
        
        debug!("Generating embedding for text of length: {}", text.len());
        
        // Generate embedding using FastEmbed
        let embeddings = self.model
            .embed(vec![text], None)
            .context("Failed to generate embedding with FastEmbed")?;
        
        if embeddings.is_empty() {
            anyhow::bail!("No embeddings generated by FastEmbed");
        }
        
        // Convert Vec<f32> to Vec<f64> for database storage
        let embedding: Vec<f64> = embeddings[0]
            .iter()
            .map(|&x| x as f64)
            .collect();
        
        debug!("Generated embedding with dimension: {}", embedding.len());
        Ok(embedding)
    }

    /// Generate embeddings for multiple texts in a single batch for improved performance
    pub async fn generate_embeddings_batch(&mut self, texts: &[String]) -> Result<Vec<Vec<f64>>> {
        if texts.is_empty() {
            return Ok(Vec::new());
        }
        
        // Filter out empty texts and track their original positions
        let mut valid_texts = Vec::new();
        let mut valid_indices = Vec::new();
        
        for (i, text) in texts.iter().enumerate() {
            if !text.trim().is_empty() {
                valid_texts.push(text.as_str());
                valid_indices.push(i);
            }
        }
        
        let valid_count = valid_texts.len();
        
        if valid_count == 0 {
            anyhow::bail!("Cannot generate embeddings for empty texts");
        }
        
        debug!("Generating embeddings for batch of {} texts", valid_count);
        
        // Generate embeddings using FastEmbed batch processing
        let embeddings = self.model
            .embed(valid_texts, None)
            .context("Failed to generate batch embeddings with FastEmbed")?;
        
        if embeddings.len() != valid_count {
            anyhow::bail!(
                "Embedding count mismatch: expected {}, got {}", 
                valid_count, 
                embeddings.len()
            );
        }
        
        // Convert all embeddings from Vec<f32> to Vec<f64> for database storage
        let mut result = vec![Vec::new(); texts.len()];
        for (valid_idx, embedding_idx) in valid_indices.iter().enumerate() {
            let embedding: Vec<f64> = embeddings[valid_idx]
                .iter()
                .map(|&x| x as f64)
                .collect();
            result[*embedding_idx] = embedding;
        }
        
        debug!("Generated {} embeddings with dimension: {}", 
               embeddings.len(), 
               embeddings.first().map(|e| e.len()).unwrap_or(0));
        
        Ok(result)
    }
    
    pub fn model_name(&self) -> &str {
        &self.model_name
    }
    
    pub fn embedding_dimension(&self) -> usize {
        // Get dimension from the actual model
        // This is a rough estimate based on common model dimensions
        match self.model_name.as_str() {
            "BAAI/bge-small-en-v1.5" => 384,
            "BAAI/bge-base-en-v1.5" => 768,
            "BAAI/bge-large-en-v1.5" => 1024,
            "sentence-transformers/all-MiniLM-L6-v2" => 384,
            "sentence-transformers/all-MiniLM-L12-v2" => 384,
            "intfloat/multilingual-e5-large" => 1024,
            _ => 384, // Default fallback
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_embedding_generation() {
        // Note: These tests require model downloads, so they may be slow on first run
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let text = "This is a test sentence for embedding generation.";
        
        let embedding = manager.generate_embedding(text).await.unwrap();
        
        assert!(!embedding.is_empty());
        assert_eq!(embedding.len(), manager.embedding_dimension());
        
        // Check that embedding values are reasonable (not all zeros)
        assert!(embedding.iter().any(|&x| x != 0.0));
        
        // FastEmbed embeddings are typically normalized
        let magnitude: f64 = embedding.iter().map(|x| x * x).sum::<f64>().sqrt();
        assert!(magnitude > 0.5); // Should be reasonably normalized
    }
    
    #[tokio::test]
    async fn test_consistent_embeddings() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let text = "This is a test sentence.";
        
        let embedding1 = manager.generate_embedding(text).await.unwrap();
        let embedding2 = manager.generate_embedding(text).await.unwrap();
        
        // FastEmbed should produce consistent embeddings for the same input
        assert_eq!(embedding1, embedding2);
    }
    
    #[tokio::test]
    async fn test_different_texts_different_embeddings() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        
        let embedding1 = manager.generate_embedding("First text").await.unwrap();
        let embedding2 = manager.generate_embedding("Second text").await.unwrap();
        
        // Different texts should produce different embeddings
        assert_ne!(embedding1, embedding2);
    }
    
    #[tokio::test]
    async fn test_empty_text_error() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let result = manager.generate_embedding("").await;
        
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_batch_embedding_generation() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let texts = vec![
            "First test sentence.".to_string(),
            "Second test sentence.".to_string(),
            "Third test sentence.".to_string(),
        ];
        
        let embeddings = manager.generate_embeddings_batch(&texts).await.unwrap();
        
        assert_eq!(embeddings.len(), texts.len());
        
        // Check that each embedding has the right dimension
        for embedding in &embeddings {
            assert!(!embedding.is_empty());
            assert_eq!(embedding.len(), manager.embedding_dimension());
        }
        
        // Check that different texts produce different embeddings
        assert_ne!(embeddings[0], embeddings[1]);
        assert_ne!(embeddings[1], embeddings[2]);
    }

    #[tokio::test]
    async fn test_batch_with_empty_texts() {
        let mut manager = EmbeddingManager::new("BAAI/bge-small-en-v1.5").await.unwrap();
        let texts = vec![
            "Valid text".to_string(),
            "".to_string(),  // Empty text
            "Another valid text".to_string(),
        ];
        
        let embeddings = manager.generate_embeddings_batch(&texts).await.unwrap();
        
        assert_eq!(embeddings.len(), texts.len());
        assert!(!embeddings[0].is_empty());  // First should have embedding
        assert!(embeddings[1].is_empty());   // Second should be empty (was empty text)
        assert!(!embeddings[2].is_empty());  // Third should have embedding
    }
}